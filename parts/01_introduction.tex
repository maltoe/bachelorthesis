\section{Introduction}
In the early days of general-purpose computing, processing power was a scarce resource. Expensive, room-filling mainframes were often shared among numerous persons and thus their utilization needed to be as efficient and time-saving as possible.   Performance, in terms of both minimum processor cycles and memory comsumption, was a critical feature for any software written in those days. Then, with the emergence of the personal computer and fast general-purpose microprocessors, application performance became far less important and programmers began to pay less attention to it when designing their applications. Today, small computer towers sitting below everyone's desk outperform all of those early mainframes and provide more than enough processing power for most everyday use cases. When it comes to creating usual desktop applications or low-scale web applications, programmers can treat both memory and CPU power as effectivly abundant resources. However, there are still plenty of cases where software performance matters to the user or even is a critical property of the application. For instance, in multimedia applications, video games, or embedded systems good performance is absolutely needed to create a pleasant user experience. In scientific computing or industry control systems, bad software performance often is intolerable.

Today, computer science lecturers tend to reduce algorithm performance to asymptotic complexity. An algorithm's performance is usually classified using the \emph{Big-O notation}, which indicates an upper bound on the growth of the number of computational ``steps'' the algorithm performs relative to the input size (referred to as $N$), while it neglects the constant amount of time needed for each step. This concept is based on the assumption that the performance impact of the so-called \emph{constant factor} decreases with growing $N$, up to the point where performance is entirely dominated by the size of the input value. The Big-O notation has proved especially valuable for comparison between algorithms, as the constant factor depends on the internals of the system executing the algorithm as well as implementation details and therefore is hard to measure. By contrast, identifying the asymptotic complexity of an algorithm is merely a matter of counting the depth of loop nestings in its implementation. However, when it comes to real-world applications, complexity theory often does not tell much about the application's performance. In real-world applications, where the input size may be small, the constant factor often plays an at least equally important role.

Sorting algorithms, a popular research topic for decades, present a very good example. Consider the well-known sorting algorithms \emph{Quicksort} and \emph{Heapsort} (for reference, consult~\cite[pp. 113--122, 144--148]{knuth1998art}): Quicksort, on the one hand, has a best-case complexity of $O(N\cdot \log{N})$ and a worst-case complexity of $O(N^{2})$, whereas, on the other hand, Heapsort provides a guaranteed complexity of $O(N\cdot \log{N})$ in all cases. Still, in most everyday use cases, especially when $N$ is small, the constant factor of Quicksort outweighs Heapsort's superior asymptotic performance. This is due to the fact that a ``step'' of the Quicksort algorithm consists of only a single comparison operation and a few data movements, whereas Heapsort requires a multitude of comparisons and moves in order to maintain the underlying data structure, a binary heap. As a consequence, Heapsort is rarely implemented.

Having emphasized the importance of the constant factor, it should have become clear that, apart from choosing the right algorithm to solve a particular problem, implementation details are crucial for a real-world application's performance. Therefore, the knowledge of various software optimization techniques remains a vital part of every programmer's profile. Knowing about how the CPU, its caches, and the memory work internally helps to understand where to find bottlenecks and how to avoid them. Some optimization techniques are mere guidelines that should be remembered and considered whenever writing a piece of software, whereas others are rather advanced measures only useful in rare and specific situations. In general, when optimizing an application, the programmer's main goal should be to use all system components to their capacity and to prevent any of them from stalling. For example, knowing about the various execution units embedded into the CPU, the programmer should try to use them all simultaneously, rather than using first one and then the other. Alike, he should keep both the CPU and the memory controller busy at most times, trying to prevent one of them waiting for the other. The programmer should particularly keep the CPU's caching mechanism in mind, as it is ntended to improve performance rather than hinder it (e.g., due to \emph{cache misses}, see Section~\ref{memory_performance}), yet it requires the application to be designed in a cache-friendly way. Most performance bottlenecks in an application result from the programmer's failure to utilize the capabilities of the system components to the fullest.

Lately, with processor speeds apparently having reached their limits at around 3-4 GHz, parallelization techniques have been brought into focus by CPU manufacturers and researchers alike. Both Intel and AMD have started to primarily ship multi-core microprocessors in their end user product segments. These processors' superior performance relies on the programmer to produce highly parallelized concurrent applications. Apart from that, most modern x86 descendants feature extended instruction sets that allow data-parallellism. These SIMD (single instruction, multiple data) instructions enable the programmer to issue only one instruction to process multiple data values at a time. Introduced by Intel for the Pentium III in 1999 and later adopted by AMD, the \emph{Streaming SIMD Extensions} (SSE) have become one of the most advanced and wide-spread SIMD technology available. While it was originally conceived to support multimedia processing and thus mainly contained multimedia specific instructions, it matured over time and became an almost fully-fledged vector processing unit. Processors implementing the latest SSE incarnation, SSE4, feature 16 \emph{media registers} called \texttt{xmm0} to \texttt{xmm15}, each of them 128 bits wide, and numerous instructions for parallel floating point and integer calculations. For example, using these, the programmer can choose to calculate the square root of four 32 bit floating point values within a single instruction which would take about the same amount of time as its scalar counterpart\footnote{Actually, the \texttt{FSQRT} instruction of the x87 FPU may even be a bit slower than the SSE version \texttt{SQRTSS/PS} (at least on an Intel ``Wolfdale'' CPU), yet provides for a greater precision as it internally operates on 80 bits. See~\cite{fog2011instructiontables} for instruction benchmarks.}. However, using SIMD extensions for optimizational work falls into the second category of optimization techniques mentioned above. These instructions only turn to account in situations where one has highly parallelized applications preferably consisting of mostly calculation intensive code.

In the following, I will show how implementing various optimization techniques can greatly improve performance of a real-world application written in C. The LS$^{2}$ simulation engine written by Heiko Will, Thomas Hillebrandt, and Marcel Kyas is a high performant algorithm simulation framework created to evaluate the accuracy of various lateration algorithms. Although the application already features highly optimized C code, average runtimes of some of the implemented algorithms still are intolerably lengthy. For further optimization, I will mainly make use of SSE instructions for parallelized floating point operations and will elaborate on the pitfalls that arise from it. Afterwards, I will assess the optimization measures' success using empiric benchmarks, which I will try to design to be statistically reliable. I will additionally describe the approach taken for development, which may provide some insight into what distinguishes optimization from implementation work.

The main goal of my work is to significantly enhance the performance of three of the algorithms included in LS$^{2}$, in particular \emph{Geolateration}, \emph{Voting Based Location Estimation}, and \emph{Adapted Multilateration}. The main part of this thesis (Section~\ref{Implementation}) will therefore be comprised of a detailed depiction and analysis of this case study. As a secondary goal, I would like this thesis to become generally useful as a guide towards SSE programming, although the extent of this will certainly be somewhat limited. Nevertheless, in Section~\ref{Optimization_theory} I will present an overview of software performance and optimization theory. Therein the reader will find some general information on modern processor architectures and how to make use of their sophisticated internals. The bulk part of Section~\ref{Optimization_theory} will describe special techniques for code vectorization, for example, how to transform conditional jumps of scalar code into logical functions and assignments that can be deployed in SSE code.

Research done for this thesis concentrated on modern Intel processor architectures and the GNU compiler collection (gcc) and may not apply to other architectures or compilers. Although, since SSE is an industry standard that all major x86 vendors have agreed on and that also is supported well by all popular compilers, it is very likely that at least the basic concepts can be transferred to other processors and compilers. As a final remark, please note that there is not a ``single way'' in software optimization and the information presented in this thesis should not be taken as a complete summary of the topic.
