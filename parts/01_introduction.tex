\section{Introduction}
In the early days of general-purpose computing, processor power and time were scarce resources. Expensive, room-filling mainframes were often shared among numerous people and thus their utilization needed to be as efficient and time-saving as possible.   Performance, in terms of both minimum processor cycles and memory comsumption, was a critical feature for any software written in those days. Then, with the emergence of the personal computer and fast general-purpose processors, application performance became far less important and programmers began to pay less attention to it when designing their applications. Today, small computer towers sitting below everyone's desk outperform all of those early mainframes and provide more than enough processor power for most everyday use cases. When it comes to creating usual desktop applications or low-scale web applications, programmers can treat both memory and CPU power as effectivly abundant resources. However, there is still a plenty of cases where software performance matters to the user or even is a critical property of the application. For instance, in multimedia applications, video games, or embedded systems good performance is mostly needed to establish a pleasant user experience. Scientific computing or industry control systems may rely on high or even real-time performance to serve their purpose at all.

Although computer science lecturers tend to reduce software performance to asymptotic complexity, the so-called \emph{constant factor} [ZITAT quicksrt] plays an at least equally important role in real-world applications. Therefore, the knowledge of various software optimization techniques remains a vital aspect of every programmer's profile. Knowing about how the CPU, its caches, and the memory work internally helps to understand where to find bottlenecks and how to avoid them. Some optimization techniques are mere guidelines that should be remembered and considered whenever writing a piece of software, whereas others are rather advanced measures only useful in special, rare situations. Lately, with processor speeds apparently having reached their limits at around 3-4 GHz, parallelization techniques have been brought into focus by CPU manufacturers and researchers alike. Both Intel and AMD have started to primarily ship multi-core processors in their end user product lines. --> SIMD

\begin{itemize}
\item motivation
\item goals
\item restrictions: only packed single, ..
\item document outline
\end{itemize}
\subsection{liblat: An evaluation framework for lateration algorithms}
\begin{itemize}
\item explain liblat; goals, functionality
\item cite heiko
\end{itemize}
\subsection{Related work}
and then there was Agner Fog~\cite{fog2011optimizing} and also Agner Fog~\cite{fog2011instructiontables}. and more.
